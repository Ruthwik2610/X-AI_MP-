Classification and Explanation with XGBoost and LIME

Project Structure:

project/

â”œâ”€â”€ data/

â”‚ â”œâ”€â”€ aifbfixed\_complete.n3

â”‚ â”œâ”€â”€ completeDataset.tsv

â”‚ â”œâ”€â”€ trainingSet.tsv

â”‚ â””â”€â”€ testSet.tsv

â”œâ”€â”€ main.ipynb # Baseline model (78% accuracy)

â”œâ”€â”€ improved.ipynb # Refined model (86% accuracy)

â”œâ”€â”€ clean.csv # Preprocessed dataset for improved model

â”œâ”€â”€ requirements.txt # Required packages

â””â”€â”€ README.txt # â† You are here

\-------------------------------------------------------------------------------------------

Prerequisites:

Ensure you have Python 3.8+ and the following packages installed:

pip install pandas numpy rdflib scikit-learn xgboost lime matplotlib seaborn

Alternatively, run: pip install -r requirements.txt

\-------------------------------------------------------------------------------------------

Dataset Setup:

â¦Make sure the following files are present in the data/ folder:

â¦aifbfixed\_complete.n3 â€“ RDF Knowledge Graph

â¦trainingSet.tsv / testSet.tsv â€“ Train/test entity URIs + labels

â¦completeDataset.tsv â€“ For label distribution visualization

\-------------------------------------------------------------------------------------------

Execution Order

Step 1: Run main.ipynb

Implements baseline classification pipeline

Uses all features after variance thresholding

Test Accuracy: ~78%

Includes:

â¦RDF parsing, preprocessing

â¦Feature selection

â¦XGBoost training + evaluation

â¦LIME local explanation

Step 2:

â¦Run improved.ipynb

â¦Builds upon main.ipynb using model-driven feature selection

â¦Uses only features with high importance scores

â¦Input data: clean.csv (filtered dataset with valid persons only)

â¦Test Accuracy: 86%

\-------------------------------------------------------------------------------------------

Running the Notebooks:

Open the notebooks with Jupyter Notebook or VSCode, and run cells sequentially.

Sections include:

â¦Setup and Imports

â¦RDF Graph Parsing and Preprocessing

â¦Data Cleaning and Label Extraction

â¦Feature Engineering + Selection

â¦Model Training using XGBoost

â¦LIME-based Explanation

â¦Class Distribution and Feature Importance Visualization

\-------------------------------------------------------------------------------------------

Key Outputs

â¦Train & Test Accuracy + Classification Report

â¦Visualized Label Distribution

â¦Top 20 Features from XGBoost

â¦LIME Explanation for a sample instance

\-------------------------------------------------------------------------------------------

Highlights of the Pipeline:

â¦Converts RDF triples into tabular format

â¦Uses VarianceThreshold for feature filtering

â¦Handles multi-class classification using XGBoost

â¦Applies LIME for instance-level explanation

â¦Improves accuracy from 78% to 86% with feature importance filtering

\-------------------------------------------------------------------------------------------

Troubleshooting:

â¦Ensure all file paths and formats match expectations

â¦Check .n3 encoding if RDF parsing fails

â¦clean.csv should exist before running improved.ipynb

â¦For best LIME results, use discretize\_continuous=True and ensure valid instances

\-------------------------------------------------------------------------------------------

References:

ğŸ”— LIME Tutorial â€“ Official Docs

https://marcotcr.github.io/lime/tutorials/Lime - basic usage, two class case.html

ğŸ”— XGBoost in Python â€“ DataCamp

https://www.datacamp.com/tutorial/xgboost-in-python

ğŸ”— Feature Selection using XGBoost â€“ Dhanya (2021)

https://medium.com/@dhanyahari07/feature-selection-using-xgboost-f0622fb70c4d

ğŸ”— AIFB Dataset â€“ DataHub

https://datahub.io/dataset/aifb
